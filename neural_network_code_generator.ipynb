{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Part of the training and accuracy check were taken from assigment-2\n",
    "\n",
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "\n",
    "def check_accuracy_part(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc\n",
    "def train_part(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on MNIST using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    best_acc = -1\n",
    "    best_model = None\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(data_train_loader):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                acc = check_accuracy_part(data_test_loader, model)\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_model = model\n",
    "                print()\n",
    "    print('Best accuracy found', best_acc)\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "if 0:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "using device: cpu\n",
      "Iteration 0, loss = 2.3014\n",
      "Checking accuracy on test set\n",
      "Got 1531 / 10000 correct (15.31)\n",
      "\n",
      "Iteration 50, loss = 1.7903\n",
      "Checking accuracy on test set\n",
      "Got 3495 / 10000 correct (34.95)\n",
      "\n",
      "Iteration 100, loss = 1.6537\n",
      "Checking accuracy on test set\n",
      "Got 4139 / 10000 correct (41.39)\n",
      "\n",
      "Iteration 150, loss = 1.4941\n",
      "Checking accuracy on test set\n",
      "Got 4434 / 10000 correct (44.34)\n",
      "\n",
      "Iteration 200, loss = 1.5200\n",
      "Checking accuracy on test set\n",
      "Got 4762 / 10000 correct (47.62)\n",
      "\n",
      "Iteration 250, loss = 1.4983\n",
      "Checking accuracy on test set\n",
      "Got 4940 / 10000 correct (49.40)\n",
      "\n",
      "Iteration 300, loss = 1.3910\n",
      "Checking accuracy on test set\n",
      "Got 5268 / 10000 correct (52.68)\n",
      "\n",
      "Iteration 350, loss = 1.2146\n",
      "Checking accuracy on test set\n",
      "Got 5431 / 10000 correct (54.31)\n",
      "\n",
      "Best accuracy found 0.5431\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(32, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (7): ReLU()\n",
      "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (9): Flatten()\n",
      "  (10): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    trainset = dset.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    data_train_loader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                              shuffle=True)\n",
    "\n",
    "    testset = dset.CIFAR10(root='./data', train=False,\n",
    "                                           download=True, transform=transform)\n",
    "    data_test_loader = torch.utils.data.DataLoader(testset, batch_size=512,\n",
    "                                             shuffle=False)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat',\n",
    "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    print_every = 50\n",
    "    print('using device:', device)\n",
    "\n",
    "    # Neural network architecture\n",
    "    # Current code only supports conv2d-ReLU-maxPool2d pairs together and Linear\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "        nn.Conv2d(32, 16, kernel_size=5, stride=1, padding=2),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "        nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "        Flatten(),\n",
    "\n",
    "        nn.Linear(4*4*32, 10),\n",
    "    )\n",
    "\n",
    "else:\n",
    "    data_train = MNIST('./data/mnist',\n",
    "                       download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize((32, 32)),\n",
    "                           transforms.ToTensor()]))\n",
    "\n",
    "    data_test = MNIST('./data/mnist',\n",
    "                      train=False,\n",
    "                      download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.Resize((32, 32)),\n",
    "                          transforms.ToTensor()]))\n",
    "\n",
    "    data_train_loader = DataLoader(data_train, batch_size=256, shuffle=True)\n",
    "    data_test_loader = DataLoader(data_test, batch_size=1024)\n",
    "\n",
    "\n",
    "    print_every = 50\n",
    "    print('using device:', device)\n",
    "\n",
    "\n",
    "    # Lenet-5 architecture\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "        nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "        Flatten(),\n",
    "        nn.Linear(5*5*16, 120),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(120, 84),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(84, 10),\n",
    "    )    \n",
    "    \n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=2e-2, momentum=0.9, nesterov=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
    "\n",
    "best_model = train_part(model, optimizer, 1)\n",
    "\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 32, 32)\n",
      "torch.Size([512, 10])\n",
      "tensor([-4.4179, -3.4760, -1.3760,  1.9832, -1.7233,  0.8797,  1.5725,  1.5281,\n",
      "        -5.1987,  0.1329], grad_fn=<SelectBackward>) \n",
      " prediction: tensor(3)\n",
      "3 32 32\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "torch.Size([512, 10])\n",
      "Conv layer\n",
      "w_0_weight_2d (32, 3, 5, 5)\n",
      "Conv layer\n",
      "w_3_weight_2d (16, 32, 5, 5)\n",
      "Conv layer\n",
      "w_6_weight_2d (32, 16, 5, 5)\n",
      "linear layer\n",
      "w_10_weight_2d (10, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAVP0lEQVR4nO3df5CdVX3H8fd392ZZlgXCkoSE/GD5ZTFlQsxsMxEzFFEzKaUFaauAdagyxmllWqY6lcEZpY5tpRaQmTrYCKngACEqSGpRwMiASElYkITfGOICCQkJP0KyrMuy2W//uDd1Q5/vs7t3733uJufzmsnk7vne8zwnT+73Pvees+ccc3dE5MDX1OgGiEgxlOwiiVCyiyRCyS6SCCW7SCKU7CKJKI2nspktBa4BmoHr3P3rec+fMmWKd3Z2ZsbyhgCHhoaC8j1hnfzjxTH37HPlHTNq34jnCiP5sf7+34axvt5dmeWtB7eFdSa1tIaxgya1hLFSKX75NDVZEIn/ZWbxvcfC40FTTr2wTlPOuSw+V7XqccwsPT09vPrqq5knqzrZzawZ+BbwEWAz8LCZrXb3p6I6nZ2ddHd3Z8b6+/vDc0Wx/v7esM7AwMCYj1dtvYGB+Hh9fTnHG8x5kwgj8MxzT4Sx7gd/nll+0rz5YZ1ZR78njM05ek4Ymz5tShhra4veJAbDOqVS/KbT0poTK439Damlpdo3sfhNoh6xserq6orPM47jLgQ2uvsmdx8AVgJnj+N4IlJH40n2mcBLw37eXCkTkQmo7h10ZrbMzLrNrHvHjh31Pp2IBMaT7FuA2cN+nlUp24e7L3f3Lnfvmjp16jhOJyLjMZ5kfxg40cyONbMW4DxgdW2aJSK1VnVvvLsPmtnFwF2Uh95WuPuTI9QJe7R37coeMoK4h3xwMO7pzushz+2NH4x7i/v6susN5tTpz2njc889F8Ye+PmdYez2O9aEsdCtPxl7HWBSzojRWX/6wTC2dMk5meXzFsS9xR1T4uHBoeDaAwyV4uvfFPSs5/2f1aOnPq9eNcerpgd/XOPs7n4nEL8qRWTC0G/QiSRCyS6SCCW7SCKU7CKJULKLJGJcvfFj5e7hkEf+zLGoPK6TN7SSG8uZCDMUvDdu3NQT1rn5u98MY/f9MnekckJ4J2f63e133Dvm2Owj4+Oddd6nwtgFF/xVGMubrDM4mD1klzdylTdMlvfayauX91qtZliuGrqziyRCyS6SCCW7SCKU7CKJULKLJGLC9Mbn955n95DnLSFVbSyn05R1Dz2QWf7FL34priT7eOm1OHbtt/4zjK26MY5dtfz6MHbaqWdkludNourvr25CS7WTa6o5VzV0ZxdJhJJdJBFKdpFEKNlFEqFkF0mEkl0kEQ0YeovWkxv7xJWB/rhOb846c3mTEu68bWUYu+LKa8OY1Ndru+PYhedfFMY+8fE/ziz/m0suC+vMmjUrjA3mvOZKOTvT1Fo0LJe37Znu7CKJULKLJELJLpIIJbtIIpTsIolQsoskYlxDb2bWA+wG9gCD7h7v7QO4D1U19BbNUuvt3RnWGcxZZOzOO+Mt6b6h4bUDyk23/ndm+d0/zi4HuG7lTWFswfxFYSznJQzkTKcsSC3G2T/o7q/W4DgiUkf6GC+SiPEmuwN3m9kjZrasFg0SkfoY78f4xe6+xcymAfeY2TPufv/wJ1TeBJYBzJw5c5ynE5FqjevO7u5bKn9vB24HFmY8Z7m7d7l715FHdozndCIyDlUnu5kdYmaH7n0MLAGeqFXDRKS2xvMx/ijgdjPbe5yb3f2nI1WKZpwNDMSz1Hp7ezPLB4nHOnqeeTGMfeOfrgxjkoYdb8Wxc//kE2Hs5lvihS8XLjo9jOUtclpLudtMVXtQd98EnFJtfREplobeRBKhZBdJhJJdJBFKdpFEKNlFElH4gpMDA8HikX3x0MTgUPaw3M7euM4/fOFTY2ucSMWenNgF58evqxXfuyGMLV58WhiLZnw25czcjGJacFJElOwiqVCyiyRCyS6SCCW7SCIK7Y0fGnL6+7N70Pv7+8J6/QPZ70n/fOklYZ0db4ytbSKjkddT//efvjCM3Xz3mjB23JzO6hv0LuqNFxElu0gqlOwiiVCyiyRCyS6SCCW7SCIKHnobCofY+nO2x/nB6pWZ5WvXP1uTdo3Wpz5wRGb5g93xON+zb9erNdmmBuUtOXW21KMhCXrtnTh22d/+ZRj79+t+lFneMfmwMbdBQ28iomQXSYWSXSQRSnaRRCjZRRKhZBdJxIhDb2a2AjgL2O7uJ1fKOoBbgU6gB/iYu484z8wdBgayh9ie2fRMWO8/rrl2pEMXojTYmll+7rl/GNb5l1vuq1dzMl1yUXZb7v5R3I4tr9WrNbLXI49vDWNXfe3LmeWXfvWrYZ3oLu052z+N5s7+XWDpu9sBrHH3E4E1lZ9FZAIbMdkr+62//q7is4G9S2neAJxT43aJSI1V+539KHff+7lkG+UdXUVkAht3B52Xfz8v/B09M1tmZt1m1r1zp5aPEWmUapP9FTObAVD5e3v0RHdf7u5d7t41eXL275aLSP1Vm+yrgb0Lbl0I3FGb5ohIvYxm6O0W4HRgipltBr4CfB1YZWYXAS8AHxvNyYaG9tDbuyszdvN13x5lkxvn5U3ZwyfTp08rtB0n5sRayB4e3KzhtQnr+/91V2b5GWeeFdY5dfHpmeVDObPeRkx2dz8/CH1opLoiMnHoN+hEEqFkF0mEkl0kEUp2kUQo2UUSUeiCk319fWzYsCEz9otf/KrIplSle0d2+dz+d08d+J1Dc463u8p25P2nbX5mXWb581WeSxpn1crrwtjceQsyy/fsiXej051dJBFKdpFEKNlFEqFkF0mEkl0kEUp2kUQUOvT21lu9PPjQA0WesqZeCcoffeylsM68aPM1YF0wlAfk7HwHLZPi2EMPpbdAyCE5sbcKa0Xt3Xvf+jD26c0vZpYPvDMQ1tGdXSQRSnaRRCjZRRKhZBdJhJJdJBGF9sa/3d/Hpqe6izxlIXbtjGOLlsTd8e/pzTno4GAY6nkm7nHfmNPDf6CamxN7uLBWFOu2lTdmlr/xerzYoO7sIolQsoskQskukgglu0gilOwiiVCyiyTCPGe7GAAzWwGcBWx395MrZZcDnwH2DvRc5u53jnSykpm3W3bszfxmTGgfydmwur0jjnV1HR/GNm7sCWMvbozXGXsuGHqLp+rs/z6eE9sYlD9Sj4ZMEO6emWWjubN/F1iaUX61u8+v/Bkx0UWksUZMdne/H4iXTxWR/cJ4vrNfbGYbzGyFmWnjdZEJrtpkvxY4HpgPbAWujJ5oZsvMrNvMuvMWZBCR+qoq2d39FXff4+5DwHeAhTnPXe7uXe7epa5/kcapKv/MbMawHz8KPFGb5ohIvYw4683MbgFOB6aY2WbgK8DpZjYfcKAH+OxoTraH/XuILdJ58uwwNmf6lDC2/al4y6v21nihuTknxMdctyNaKe/AFa+6BtsKa8XEN2Kyu/v5GcXX16EtIlJH+hotkgglu0gilOwiiVCyiyRCyS6SiEIXnDxQ9ecsDrlgyXlhbN3g5DA2b9GCMPbggw+Gsd3hJlW1d2ROLF72sPZuL/Bc+zPd2UUSoWQXSYSSXSQRSnaRRCjZRRKhZBdJhIbexiBaV7ItZ97Vtm0vh7HOhYvCWM+2njD26AP/E8aKVOvhtZx1O5le5THXV1nvQKQ7u0gilOwiiVCyiyRCyS6SCCW7SCLUGz8G04Jl4To64j2eOkp9YayvL14hbfNjPwtjbfvB/1q0sVU8NgHx1B+YlxPrPSSOvf5WdvmBvB1WRHd2kUQo2UUSoWQXSYSSXSQRSnaRRCjZRRIxmu2fZgM3Up6n4MByd7/GzDqAW4FOyltAfczd38g7VkupxMwjslcu+81+sG3R4+9kl7f/9NdhnbZSfIkHcibQvL4tnmbStysMFerPchahO+GE7PIr1sZ1Jse7aNGaMxPmuofjWJFr4U10o7mzDwKfd/e5wCLgc2Y2F7gUWOPuJwJrKj+LyAQ1YrK7+1Z3f7TyeDfwNDATOBu4ofK0G4Bz6tVIERm/MX1nN7NO4H3AWuAod99aCW0jfzqyiDTYqH/x0szagR8Cl7j7LjP7v5i7u5llbsZsZsuAZQDNTeoPFGmUUWWfmU2inOg3ufttleJXzGxGJT4D2J5V192Xu3uXu3cp2UUaZ8Tss/It/HrgaXe/alhoNXBh5fGFwB21b56I1Iq5Z376/t0TzBYDvwAeB4YqxZdR/t6+CpgDvEB56O31vGMdftjh/v6u92fG7rr3rjE1fH9xYk6s9aA4tvHtONafc8z8/82xm5ETm394HPtwe3b5bVviOqfmnCveYAuuzomlyN0tq3zE7+zu/gCQWRn40HgaJSLF0ZdokUQo2UUSoWQXSYSSXSQRSnaRRBS6dGFzc4nJHdOKPGXDxfPhwHKG14JJYwC0Nsexx/eM1KL/L+/3nD+cE9v1Zk4wiHXlVDksJ3Z/TixFxxyRPe65dVdvWEd3dpFEKNlFEqFkF0mEkl0kEUp2kUQo2UUSUejQW1NTE21tbZmxIyfFG3a99k6wYdd+Lm+G2os5sbac4bVoll2841y+eElMOPWYOBYNsG57Ia7z05xz/TInlqKTFmTPEXxjXXyldGcXSYSSXSQRSnaRRCjZRRKhZBdJRKG98WZNtJRaM2MLTj0trHfPfT+pV5MmrJw5Mrmx3P23qnBrXiynZ/0PgvK8aVDqcd9XtBYcwGGTOzLLm5vjlNadXSQRSnaRRCjZRRKhZBdJhJJdJBFKdpFEjDj0ZmazgRspL1XmwHJ3v8bMLgc+A+yoPPUyd78z71hNZrS0tGTGOt8Tr7p2yqu/n1m+/sknR2i9NMrDjW7AAeCkY98bxkrBJqmWM143mnH2QeDz7v6omR0KPGJm91RiV7v7v43iGCLSYKPZ620rsLXyeLeZPQ3MrHfDRKS2xvSd3cw6gfdR3sEV4GIz22BmK8zsiBq3TURqaNTJbmbtwA+BS9x9F3AtcDwwn/Kd/8qg3jIz6zaz7t/2/7YGTRaRaowq2c1sEuVEv8ndbwNw91fcfY+7DwHfARZm1XX35e7e5e5dB7ceXKt2i8gYjZjsZmbA9cDT7n7VsPIZw572UeCJ2jdPRGplNL3xHwA+CTxuZo9Vyi4Dzjez+ZSH43qAz450oOZSiY6O7Nk6r+7cHtab37U4s/zlnpfDOjveqvX8L5H6OGbqkZnls+Z0hnVKpeA+nTP2Npre+AfInm2XO6YuIhOLfoNOJBFKdpFEKNlFEqFkF0mEkl0kEcUuONnUREtbe2asfaA/rNfWkl1nydJzwzp3/2xVGNvx5u4wJlIPMw49PIzNX5g9tNwUzGwD2NW7M7N8z57BsI7u7CKJULKLJELJLpIIJbtIIpTsIolQsoskotCht6amJtra2jJjQwOHhfV6e3dllndMy55BB7DkzAvCWPe6h8LYs8+vD2MieWZPPSqMdR43L4xFM9haWrMXZwWYM2tOZvnanNe27uwiiVCyiyRCyS6SCCW7SCKU7CKJULKLJKLYWW/WRKmUPZzQGgzJAQwNZZdH+10BMDAQhhYt7Apj06fHw3ndD92bWf7WnrgZcmA55b2/F8bmTO8MY5MPi19XpcOyc2La9GlhnWlTjs4+VmlSWEd3dpFEKNlFEqFkF0mEkl0kEUp2kUSM2BtvZq3A/cBBlef/wN2/YmbHAiuBI4FHgE+6e9wFDjRZE60t2b3upab4l/7b27N7Mne+vi0+Vynnn7Yzfo+bMyd7ggFAR8dfZJY/taE7rPPsC7+J2yENc/zMGWFs4amnhbFZR2f3ggMM9vaFsVJraxhrCia8TOmIe+Nbgjwyi1/bo7mzvw2c4e6nUN6eeamZLQKuAK529xOAN4CLRnEsEWmQEZPdy3orP06q/HHgDOAHlfIbgHPq0kIRqYnR7s/eXNnBdTtwD/A8sNPd965buxmYWZ8mikgtjCrZ3X2Pu88HZgELgZNGewIzW2Zm3WbWvbv3zSqbKSLjNabeeHffCdwLvB+YbGZ7e8FmAVuCOsvdvcvduw5tjxfKF5H6GjHZzWyqmU2uPD4Y+AjwNOWk//PK0y4E7qhXI0Vk/EYzEWYGcIOZNVN+c1jl7j82s6eAlWb2NeBXwPWjOWG0pU1rztDEUDATprU1njzT0hIP5bW1xTFKwawboCW4XPNPXhDWOWnu3DDW198bxnbtjGMvbtwUxrbufiOMHajee8yxYezkk7PXfpszZ1ZYp70jnrTSlHN/7I9fOrkTvdo7pmeXB1ullY+XnS/Nzc1hnRGT3d03AO/LKN9E+fu7iOwH9Bt0IolQsoskQskukgglu0gilOwiiTB3L+5kZjuAFyo/TgFeLezkMbVjX2rHvva3dhzj7lOzAoUm+z4nNut293jlR7VD7VA7atoOfYwXSYSSXSQRjUz25Q0893Bqx77Ujn0dMO1o2Hd2ESmWPsaLJKIhyW5mS83sWTPbaGaXNqINlXb0mNnjZvaYmcWrRtb+vCvMbLuZPTGsrMPM7jGzX1f+PqJB7bjczLZUrsljZnZmAe2YbWb3mtlTZvakmf1dpbzQa5LTjkKviZm1mtk6M1tfacc/VsqPNbO1lby51cxypm9mcPdC/wDNlJe1Og5oAdYDc4tuR6UtPcCUBpz3NGAB8MSwsn8FLq08vhS4okHtuBz4QsHXYwawoPL4UOA5YG7R1ySnHYVeE8CA9srjScBaYBGwCjivUv5t4K/HctxG3NkXAhvdfZOXl55eCZzdgHY0jLvfD7z+ruKzKS/cCQUt4Bm0o3DuvtXdH6083k15cZSZFHxNctpRKC+r+SKvjUj2mcBLw35u5GKVDtxtZo+Y2bIGtWGvo9x9a+XxNuCoBrblYjPbUPmYX/evE8OZWSfl9RPW0sBr8q52QMHXpB6LvKbeQbfY3RcAfwR8zszi3QEK5OXPaY0aJrkWOJ7yHgFbgSuLOrGZtQM/BC5x913DY0Vek4x2FH5NfByLvEYakexbgNnDfg4Xq6w3d99S+Xs7cDuNXXnnFTObAVD5e3sjGuHur1ReaEPAdyjompjZJMoJdpO731YpLvyaZLWjUdekcu4xL/IaaUSyPwycWOlZbAHOA1YX3QgzO8TMDt37GFgCPJFfq65WU164Exq4gOfe5Kr4KAVcEzMzymsYPu3uVw0LFXpNonYUfU3qtshrUT2M7+ptPJNyT+fzwJca1IbjKI8ErAeeLLIdwC2UPw6+Q/m710WU98xbA/wa+BnQ0aB2fA94HNhAOdlmFNCOxZQ/om8AHqv8ObPoa5LTjkKvCTCP8iKuGyi/sXx52Gt2HbAR+D5w0FiOq9+gE0lE6h10IslQsoskQskukgglu0gilOwiiVCyiyRCyS6SCCW7SCL+F3oa4/p4p1OhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# weights header and network generator. This generates main.h and main.c\n",
    "weights_file = open('main.h', 'w')\n",
    "weights_file.write('typedef float data_t;\\n\\n')\n",
    "\n",
    "c_file = open('main.c', 'w')\n",
    "c_file.write('#include <stdio.h>\\n\\\n",
    "#include <string.h>\\n\\\n",
    "#include <stdint.h>\\n\\\n",
    "#include \"main.h\"\\n\\\n",
    "#include \"helper_functions.h\"\\n\\\n",
    "\\n')\n",
    "\n",
    "c_file.write('\\n\\\n",
    "int main()\\n\\\n",
    "{\\n')\n",
    "\n",
    "\n",
    "sample_vector_batch = None\n",
    "\n",
    "for i, (images, labels) in enumerate(data_test_loader):\n",
    "    if i == 1:\n",
    "        selected_in_batch = 3\n",
    "\n",
    "        img = images[selected_in_batch].numpy()\n",
    "        print(img.shape)\n",
    "        #plt.imshow(img.reshape(32,32), vmin=0, vmax=1)\n",
    "        plt.imshow(img.transpose(1,2,0), vmin=0, vmax=1)\n",
    "        \n",
    "        sample_vector_batch = images\n",
    "        \n",
    "        weights_file.write('const data_t test['+str(img.size)+']={')\n",
    "        np.savetxt(weights_file, images[selected_in_batch].flatten(), newline=',')\n",
    "        weights_file.write('};\\n')\n",
    "\n",
    "        images = images.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "        #output = lenet_debug(images)\n",
    "        output = best_model(images)\n",
    "        print(output.shape)\n",
    "        _, preds = output.max(axis=1)\n",
    "        #print(output[selected_in_batch, 0])\n",
    "        print(output[selected_in_batch], '\\n prediction:', preds[selected_in_batch])\n",
    "        break\n",
    "\n",
    "result = sample_vector_batch\n",
    "input_size = result.shape[1]*result.shape[2]*result.shape[3]\n",
    "L = np.empty(0)\n",
    "L = np.append(L, np.uint32(input_size))\n",
    "\n",
    "print(result.shape[1], result.shape[2], result.shape[3])\n",
    "\n",
    "previous_padding = None\n",
    "\n",
    "index = 0\n",
    "\n",
    "meta_list = list()\n",
    "\n",
    "c_file.write('\\t twoD_t meta_data'+str(index)+' = {\\n\\\n",
    "                           .r = '+ str(result.shape[2]) +',\\n\\\n",
    "                           .c = '+str(result.shape[3])+',\\n\\\n",
    "                           .channel = '+str(result.shape[1])+',\\n\\\n",
    "                           .data = buffer'+str(index%2)+',\\n\\\n",
    "                           .bias = NULL\\n\\\n",
    "                       };\\n\\n')\n",
    "\n",
    "meta_list.append(('meta_data'+str(index),(0,0,0)))\n",
    "\n",
    "for i in best_model:\n",
    "    result = i(result)\n",
    "    if 'Conv2d' in str(i):\n",
    "        previous_padding = i.padding[0]\n",
    "    if 'ReLU' in str(i):\n",
    "        continue\n",
    "    if 'MaxPool' in str(i):\n",
    "        index += 1\n",
    "        print(i.stride, i.kernel_size)\n",
    "        L = np.append(L, result.shape[1]*result.shape[2]*result.shape[3])\n",
    "        c_file.write('\\t twoD_t meta_data'+str(index)+' = {\\n\\\n",
    "                           .r = '+str(result.shape[2])+',\\n\\\n",
    "                           .c = '+str(result.shape[3])+',\\n\\\n",
    "                           .channel = '+str(result.shape[1])+',\\n\\\n",
    "                           .data = buffer'+str(index%2)+',\\n\\\n",
    "                           .bias = NULL\\n\\\n",
    "                       };\\n\\n')        \n",
    "        meta_list.append(('meta_data'+str(index),(i.stride, i.kernel_size, previous_padding)))\n",
    "    \n",
    "    if 'Linear' in str(i):\n",
    "        index += 1\n",
    "        print(result.shape)\n",
    "        L = np.append(L, result.shape[1])\n",
    "        c_file.write('\\t twoD_t meta_data'+str(index)+' = {\\n\\\n",
    "                           .r = '+str(result.shape[1]) +',\\n\\\n",
    "                           .c = 1,\\n\\\n",
    "                           .channel = 1,\\n\\\n",
    "                           .data = buffer'+str(index%2)+',\\n\\\n",
    "                           .bias = NULL\\n\\\n",
    "                       };\\n\\n')\n",
    "        meta_list.append(('meta_data'+str(index),(0,0,0)))\n",
    "\n",
    "c_file.write('\\n\\t memcpy(buffer0, test, sizeof(test));\\n')\n",
    "\n",
    "c_file.write('\\n\\t printf(\"---Network starts---\\\\n\");\\n')\n",
    "\n",
    "\n",
    "inx = np.argsort(L)\n",
    "\n",
    "weights_file.write('\\n\\\n",
    "data_t buffer'+str(inx[-1]%2)+'['+str(int(L[inx[-1]]))+'];\\n\\\n",
    "data_t buffer'+str((inx[-1]+1)%2)+'['+str(int(L[inx[-2]]))+'];\\n\\\n",
    "\\n\\\n",
    "typedef struct twoD\\n\\\n",
    "{\\n\\\n",
    "\tuint32_t r;\\n\\\n",
    "\tuint32_t c;\\n\\\n",
    "\tuint32_t in_channel;\\n\\\n",
    "\tuint32_t channel;\\n\\\n",
    "\tdata_t *data;\\n\\\n",
    "\tdata_t *bias;\\n\\\n",
    "} twoD_t;\\n\\n')\n",
    "\n",
    "prev_shapes = None\n",
    "prev_arr_name = None\n",
    "\n",
    "index = 0\n",
    "for name, param in best_model.state_dict().items():\n",
    "\n",
    "    arr = param.cpu().numpy();\n",
    "    shape_of_params = arr.shape\n",
    "    param_size =  len(arr.flatten())\n",
    "    #print(name, 'param size:', param_size)\n",
    "    underscore_arr_name = 'w_'+name.replace('.', '_')\n",
    "    array_name  = underscore_arr_name + '[' + str(param_size) + ']=' \n",
    "    weights_file.write('const data_t '+array_name+'{')\n",
    "    np.savetxt(weights_file, arr.flatten(), newline=',')\n",
    "    weights_file.write('};\\n\\n')\n",
    "\n",
    "    if len(shape_of_params) == 1:\n",
    "        index += 1\n",
    "        if len(prev_shapes) > 2:\n",
    "            print('Conv layer')\n",
    "            out_channel = prev_shapes[0]\n",
    "            in_channel = prev_shapes[1]\n",
    "            c_file.write('\\t conv2D(&'+meta_list[index-1][0]+', &'+prev_arr_name+'_2d, &'+meta_list[index][0]+', &reLU, '+str(meta_list[index][1][0])+', '+str(meta_list[index][1][1])+', '+str(meta_list[index][1][1])+', '+str(meta_list[index][1][2])+');\\n')\n",
    "            \n",
    "        else:\n",
    "            print('linear layer')\n",
    "            out_channel = 1\n",
    "            in_channel = 1\n",
    "            if len(meta_list) == (index+1):\n",
    "                c_file.write('\\t dot(&'+meta_list[index-1][0]+', &'+prev_arr_name+'_2d, &'+meta_list[index][0]+', NULL);\\n')\n",
    "            else:\n",
    "                c_file.write('\\t dot(&'+meta_list[index-1][0]+', &'+prev_arr_name+'_2d, &'+meta_list[index][0]+', &reLU);\\n')\n",
    "                \n",
    "        weights_file.write('const twoD_t '+prev_arr_name+'_2d = {\\n\\\n",
    "                           .r = '+ str(prev_shapes[-1]) +',\\n\\\n",
    "                           .c = '+str(prev_shapes[-2])+',\\n\\\n",
    "                           .in_channel = '+str(in_channel)+',\\n\\\n",
    "                           .channel = '+str(out_channel)+',\\n\\\n",
    "                           .data = '+prev_arr_name+',\\n\\\n",
    "                           .bias = '+underscore_arr_name+'\\n\\\n",
    "                       };\\n\\n')\n",
    "        print(prev_arr_name+'_2d', prev_shapes)\n",
    "\n",
    "    prev_shapes = shape_of_params\n",
    "    prev_arr_name = underscore_arr_name\n",
    "\n",
    "c_file.write('\\n\\t print_twoD(&'+meta_list[-1][0]+', 0);\\n')\n",
    "c_file.write('\\t printf(\"PREDICTION: %d\\\\n\", get_class(&'+meta_list[-1][0]+'));\\n')\n",
    "\n",
    "class network_partial(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(network_partial, self).__init__()\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:-6])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "intermediate_results= network_partial(best_model)\n",
    "\n",
    "weights_file.close()\n",
    "\n",
    "c_file.write('\\t return 0;\\n}')\n",
    "\n",
    "c_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
