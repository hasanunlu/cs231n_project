{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Part of the training and accuracy check were taken from assigment-2\n",
    "\n",
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "\n",
    "def check_accuracy_part(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc\n",
    "def train_part(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on MNIST using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    best_acc = -1\n",
    "    best_model = None\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(data_train_loader):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                acc = check_accuracy_part(data_test_loader, model)\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_model = model\n",
    "                print()\n",
    "    print('Best accuracy found', best_acc)\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "Iteration 0, loss = 2.3018\n",
      "Checking accuracy on test set\n",
      "Got 1178 / 10000 correct (11.78)\n",
      "\n",
      "Iteration 50, loss = 0.4600\n",
      "Checking accuracy on test set\n",
      "Got 8656 / 10000 correct (86.56)\n",
      "\n",
      "Iteration 100, loss = 0.3664\n",
      "Checking accuracy on test set\n",
      "Got 9200 / 10000 correct (92.00)\n",
      "\n",
      "Iteration 150, loss = 0.2107\n",
      "Checking accuracy on test set\n",
      "Got 9509 / 10000 correct (95.09)\n",
      "\n",
      "Iteration 200, loss = 0.1337\n",
      "Checking accuracy on test set\n",
      "Got 9535 / 10000 correct (95.35)\n",
      "\n",
      "Iteration 0, loss = 0.0817\n",
      "Checking accuracy on test set\n",
      "Got 9635 / 10000 correct (96.35)\n",
      "\n",
      "Iteration 50, loss = 0.1111\n",
      "Checking accuracy on test set\n",
      "Got 9728 / 10000 correct (97.28)\n",
      "\n",
      "Iteration 100, loss = 0.1087\n",
      "Checking accuracy on test set\n",
      "Got 9742 / 10000 correct (97.42)\n",
      "\n",
      "Iteration 150, loss = 0.0916\n",
      "Checking accuracy on test set\n",
      "Got 9783 / 10000 correct (97.83)\n",
      "\n",
      "Iteration 200, loss = 0.1311\n",
      "Checking accuracy on test set\n",
      "Got 9778 / 10000 correct (97.78)\n",
      "\n",
      "Iteration 0, loss = 0.0503\n",
      "Checking accuracy on test set\n",
      "Got 9809 / 10000 correct (98.09)\n",
      "\n",
      "Iteration 50, loss = 0.0427\n",
      "Checking accuracy on test set\n",
      "Got 9807 / 10000 correct (98.07)\n",
      "\n",
      "Iteration 100, loss = 0.0323\n",
      "Checking accuracy on test set\n",
      "Got 9826 / 10000 correct (98.26)\n",
      "\n",
      "Iteration 150, loss = 0.0251\n",
      "Checking accuracy on test set\n",
      "Got 9835 / 10000 correct (98.35)\n",
      "\n",
      "Iteration 200, loss = 0.0665\n",
      "Checking accuracy on test set\n",
      "Got 9855 / 10000 correct (98.55)\n",
      "\n",
      "Iteration 0, loss = 0.0722\n",
      "Checking accuracy on test set\n",
      "Got 9870 / 10000 correct (98.70)\n",
      "\n",
      "Iteration 50, loss = 0.0192\n",
      "Checking accuracy on test set\n",
      "Got 9853 / 10000 correct (98.53)\n",
      "\n",
      "Iteration 100, loss = 0.0343\n",
      "Checking accuracy on test set\n",
      "Got 9861 / 10000 correct (98.61)\n",
      "\n",
      "Iteration 150, loss = 0.0691\n",
      "Checking accuracy on test set\n",
      "Got 9880 / 10000 correct (98.80)\n",
      "\n",
      "Iteration 200, loss = 0.0595\n",
      "Checking accuracy on test set\n",
      "Got 9844 / 10000 correct (98.44)\n",
      "\n",
      "Best accuracy found 0.988\n",
      "Sequential(\n",
      "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Flatten()\n",
      "  (7): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (10): ReLU()\n",
      "  (11): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data_train = MNIST('./data/mnist',\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize((32, 32)),\n",
    "                       transforms.ToTensor()]))\n",
    "\n",
    "data_test = MNIST('./data/mnist',\n",
    "                  train=False,\n",
    "                  download=True,\n",
    "                  transform=transforms.Compose([\n",
    "                      transforms.Resize((32, 32)),\n",
    "                      transforms.ToTensor()]))\n",
    "\n",
    "data_train_loader = DataLoader(data_train, batch_size=256, shuffle=True)\n",
    "data_test_loader = DataLoader(data_test, batch_size=1024)\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print_every = 50\n",
    "print('using device:', device)\n",
    "\n",
    "\n",
    "# Lenet-5 architecture\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "    nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "    Flatten(),\n",
    "    nn.Linear(5*5*16, 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84, 10),\n",
    ")\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=2e-2, momentum=0.9, nesterov=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
    "\n",
    "best_model = train_part(model, optimizer, 4)\n",
    "\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_0_weight_2d (6, 1, 5, 5)\n",
      "w_3_weight_2d (16, 6, 5, 5)\n",
      "w_7_weight_2d (120, 400)\n",
      "w_9_weight_2d (84, 120)\n",
      "w_11_weight_2d (10, 84)\n",
      "torch.Size([1024, 10])\n",
      "tensor([10.8341, -7.5770,  0.1185, -5.0662,  0.3700, -2.4806,  3.1035, -7.5978,\n",
      "        -2.7473,  0.5504], device='cuda:0', grad_fn=<SelectBackward>) \n",
      " prediction: tensor(0, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# weights header generator. This generates main.h\n",
    "\n",
    "weights_file = open('main.h', 'w')\n",
    "\n",
    "weights_file.write('typedef float data_t;\\n\\\n",
    "\\n\\\n",
    "typedef struct twoD\\n\\\n",
    "{\\n\\\n",
    "\tuint32_t r;\\n\\\n",
    "\tuint32_t c;\\n\\\n",
    "\tuint32_t in_channel;\\n\\\n",
    "    uint32_t channel;\\n\\\n",
    "\tdata_t *data;\\n\\\n",
    "\tdata_t *bias;\\n\\\n",
    "} twoD_t;\\n\\n')\n",
    "\n",
    "prev_shapes = None\n",
    "prev_arr_name = None\n",
    "\n",
    "for name, param in best_model.state_dict().items():\n",
    "    arr = param.cpu().numpy();\n",
    "    shape_of_params = arr.shape\n",
    "    param_size =  len(arr.flatten())\n",
    "    #print(name, 'param size:', param_size)\n",
    "    underscore_arr_name = 'w_'+name.replace('.', '_')\n",
    "    array_name  = underscore_arr_name + '[' + str(param_size) + ']=' \n",
    "    weights_file.write('const data_t '+array_name+'{')\n",
    "    np.savetxt(weights_file, arr.flatten(), newline=',')\n",
    "    weights_file.write('};\\n\\n')\n",
    "    if len(shape_of_params) == 1:\n",
    "        if len(prev_shapes) > 2:\n",
    "            out_channel = prev_shapes[0]\n",
    "            in_channel = prev_shapes[1]\n",
    "        else:\n",
    "            out_channel = 1\n",
    "            in_channel = 1\n",
    "\n",
    "        weights_file.write('const twoD_t '+prev_arr_name+'_2d = {\\n\\\n",
    "                           .r ='+ str(prev_shapes[-1]) +',\\n\\\n",
    "                           .c = '+str(prev_shapes[-2])+',\\n\\\n",
    "                           .in_channel = '+str(in_channel)+',\\n\\\n",
    "                           .channel = '+str(out_channel)+',\\n\\\n",
    "                           .data = '+prev_arr_name+',\\n\\\n",
    "                           .bias = '+underscore_arr_name+'\\n\\\n",
    "                       };\\n\\n')\n",
    "        print(prev_arr_name+'_2d', prev_shapes)\n",
    "    prev_shapes = shape_of_params\n",
    "    prev_arr_name = underscore_arr_name\n",
    "\n",
    "    \n",
    "class lenet5_partial(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(lenet5_partial, self).__init__()\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:-6])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "lenet_debug = lenet5_partial(best_model)\n",
    "\n",
    "\n",
    "for i, (images, labels) in enumerate(data_test_loader):\n",
    "    if i == 6:\n",
    "        selected_in_batch = 106\n",
    "        img = images[selected_in_batch].numpy()\n",
    "        plt.imshow(img.reshape(32,32), cmap='gray', vmin=0, vmax=1)\n",
    "        \n",
    "        weights_file.write('const data_t test[32*32]={')\n",
    "        np.savetxt(weights_file, images[selected_in_batch].flatten(), newline=',')\n",
    "        weights_file.write('};\\n')\n",
    "\n",
    "        images = images.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "        #output = lenet_debug(images)\n",
    "        output = best_model(images)\n",
    "        print(output.shape)\n",
    "        _, preds = output.max(axis=1)\n",
    "        #print(output[selected_in_batch, 0])\n",
    "        print(output[selected_in_batch], '\\n prediction:', preds[selected_in_batch])\n",
    "        break\n",
    "\n",
    "weights_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
