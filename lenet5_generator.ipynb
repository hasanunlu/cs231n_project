{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Part of the training and accuracy check were taken from assigment-2\n",
    "\n",
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "\n",
    "def check_accuracy_part(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc\n",
    "def train_part(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on MNIST using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    best_acc = -1\n",
    "    best_model = None\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(data_train_loader):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                acc = check_accuracy_part(data_test_loader, model)\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_model = model\n",
    "                print()\n",
    "    print('Best accuracy found', best_acc)\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "Iteration 0, loss = 2.3027\n",
      "Checking accuracy on test set\n",
      "Got 1032 / 10000 correct (10.32)\n",
      "\n",
      "Iteration 50, loss = 0.4165\n",
      "Checking accuracy on test set\n",
      "Got 8942 / 10000 correct (89.42)\n",
      "\n",
      "Iteration 100, loss = 0.1978\n",
      "Checking accuracy on test set\n",
      "Got 9389 / 10000 correct (93.89)\n",
      "\n",
      "Iteration 150, loss = 0.1421\n",
      "Checking accuracy on test set\n",
      "Got 9555 / 10000 correct (95.55)\n",
      "\n",
      "Iteration 200, loss = 0.1490\n",
      "Checking accuracy on test set\n",
      "Got 9654 / 10000 correct (96.54)\n",
      "\n",
      "Iteration 0, loss = 0.1185\n",
      "Checking accuracy on test set\n",
      "Got 9663 / 10000 correct (96.63)\n",
      "\n",
      "Iteration 50, loss = 0.0883\n",
      "Checking accuracy on test set\n",
      "Got 9722 / 10000 correct (97.22)\n",
      "\n",
      "Iteration 100, loss = 0.0860\n",
      "Checking accuracy on test set\n",
      "Got 9745 / 10000 correct (97.45)\n",
      "\n",
      "Iteration 150, loss = 0.0492\n",
      "Checking accuracy on test set\n",
      "Got 9789 / 10000 correct (97.89)\n",
      "\n",
      "Iteration 200, loss = 0.0580\n",
      "Checking accuracy on test set\n",
      "Got 9775 / 10000 correct (97.75)\n",
      "\n",
      "Iteration 0, loss = 0.0758\n",
      "Checking accuracy on test set\n",
      "Got 9789 / 10000 correct (97.89)\n",
      "\n",
      "Iteration 50, loss = 0.1329\n",
      "Checking accuracy on test set\n",
      "Got 9833 / 10000 correct (98.33)\n",
      "\n",
      "Iteration 100, loss = 0.1637\n",
      "Checking accuracy on test set\n",
      "Got 9814 / 10000 correct (98.14)\n",
      "\n",
      "Iteration 150, loss = 0.0642\n",
      "Checking accuracy on test set\n",
      "Got 9856 / 10000 correct (98.56)\n",
      "\n",
      "Iteration 200, loss = 0.0439\n",
      "Checking accuracy on test set\n",
      "Got 9844 / 10000 correct (98.44)\n",
      "\n",
      "Iteration 0, loss = 0.0648\n",
      "Checking accuracy on test set\n",
      "Got 9846 / 10000 correct (98.46)\n",
      "\n",
      "Iteration 50, loss = 0.0439\n",
      "Checking accuracy on test set\n",
      "Got 9853 / 10000 correct (98.53)\n",
      "\n",
      "Iteration 100, loss = 0.0378\n",
      "Checking accuracy on test set\n",
      "Got 9892 / 10000 correct (98.92)\n",
      "\n",
      "Iteration 150, loss = 0.0305\n",
      "Checking accuracy on test set\n",
      "Got 9847 / 10000 correct (98.47)\n",
      "\n",
      "Iteration 200, loss = 0.0524\n",
      "Checking accuracy on test set\n",
      "Got 9856 / 10000 correct (98.56)\n",
      "\n",
      "Best accuracy found 0.9892\n",
      "Sequential(\n",
      "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Flatten()\n",
      "  (7): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (10): ReLU()\n",
      "  (11): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data_train = MNIST('./data/mnist',\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize((32, 32)),\n",
    "                       transforms.ToTensor()]))\n",
    "\n",
    "data_test = MNIST('./data/mnist',\n",
    "                  train=False,\n",
    "                  download=True,\n",
    "                  transform=transforms.Compose([\n",
    "                      transforms.Resize((32, 32)),\n",
    "                      transforms.ToTensor()]))\n",
    "\n",
    "data_train_loader = DataLoader(data_train, batch_size=256, shuffle=True)\n",
    "data_test_loader = DataLoader(data_test, batch_size=1024)\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print_every = 50\n",
    "print('using device:', device)\n",
    "\n",
    "\n",
    "# Lenet-5 architecture\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "    nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "    Flatten(),\n",
    "    nn.Linear(5*5*16, 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84, 10),\n",
    ")\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=2e-2, momentum=0.9, nesterov=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
    "\n",
    "best_model = train_part(model, optimizer, 4)\n",
    "\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_0_weight_2d (6, 1, 5, 5)\n",
      "w_3_weight_2d (16, 6, 5, 5)\n",
      "w_7_weight_2d (120, 400)\n",
      "w_9_weight_2d (84, 120)\n",
      "w_11_weight_2d (10, 84)\n",
      "torch.Size([1024, 10])\n",
      "tensor([ 1.5575, -1.0157, -2.1311, -6.8397,  1.8811,  2.1535, 11.7107, -5.0137,\n",
      "        -4.1270, -4.9690], device='cuda:0', grad_fn=<SelectBackward>) \n",
      " prediction: tensor(6, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# weights header generator. This generates main.h\n",
    "\n",
    "weights_file = open('main.h', 'w')\n",
    "\n",
    "weights_file.write('typedef float data_t;\\n\\\n",
    "\\n\\\n",
    "typedef struct twoD\\n\\\n",
    "{\\n\\\n",
    "\tuint32_t r;\\n\\\n",
    "\tuint32_t c;\\n\\\n",
    "\tuint32_t in_channel;\\n\\\n",
    "    uint32_t channel;\\n\\\n",
    "\tdata_t *data;\\n\\\n",
    "\tdata_t *bias;\\n\\\n",
    "} twoD_t;\\n\\n')\n",
    "\n",
    "prev_shapes = None\n",
    "prev_arr_name = None\n",
    "\n",
    "for name, param in best_model.state_dict().items():\n",
    "    arr = param.cpu().numpy();\n",
    "    shape_of_params = arr.shape\n",
    "    param_size =  len(arr.flatten())\n",
    "    #print(name, 'param size:', param_size)\n",
    "    underscore_arr_name = 'w_'+name.replace('.', '_')\n",
    "    array_name  = underscore_arr_name + '[' + str(param_size) + ']=' \n",
    "    weights_file.write('const data_t '+array_name+'{')\n",
    "    np.savetxt(weights_file, arr.flatten(), newline=',')\n",
    "    weights_file.write('};\\n\\n')\n",
    "    if len(shape_of_params) == 1:\n",
    "        if len(prev_shapes) > 2:\n",
    "            out_channel = prev_shapes[0]\n",
    "            in_channel = prev_shapes[1]\n",
    "        else:\n",
    "            out_channel = 1\n",
    "            in_channel = 1\n",
    "\n",
    "        weights_file.write('const twoD_t '+prev_arr_name+'_2d = {\\n\\\n",
    "                           .r ='+ str(prev_shapes[-1]) +',\\n\\\n",
    "                           .c = '+str(prev_shapes[-2])+',\\n\\\n",
    "                           .in_channel = '+str(in_channel)+',\\n\\\n",
    "                           .channel = '+str(out_channel)+',\\n\\\n",
    "                           .data = '+prev_arr_name+',\\n\\\n",
    "                           .bias = '+underscore_arr_name+'\\n\\\n",
    "                       };\\n\\n')\n",
    "        print(prev_arr_name+'_2d', prev_shapes)\n",
    "    prev_shapes = shape_of_params\n",
    "    prev_arr_name = underscore_arr_name\n",
    "\n",
    "    \n",
    "class lenet5_partial(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(lenet5_partial, self).__init__()\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:-6])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "lenet_debug = lenet5_partial(best_model)\n",
    "\n",
    "\n",
    "for i, (images, labels) in enumerate(data_test_loader):\n",
    "    if i == 6:\n",
    "        selected_in_batch = 108\n",
    "        img = images[selected_in_batch].numpy()\n",
    "        plt.imshow(img.reshape(32,32), cmap='gray', vmin=0, vmax=1)\n",
    "        \n",
    "        weights_file.write('const data_t test[32*32]={')\n",
    "        np.savetxt(weights_file, images[selected_in_batch].flatten(), newline=',')\n",
    "        weights_file.write('};\\n')\n",
    "\n",
    "        images = images.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "        #output = lenet_debug(images)\n",
    "        output = best_model(images)\n",
    "        print(output.shape)\n",
    "        _, preds = output.max(axis=1)\n",
    "        #print(output[selected_in_batch, 0])\n",
    "        print(output[selected_in_batch], '\\n prediction:', preds[selected_in_batch])\n",
    "        break\n",
    "\n",
    "weights_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
